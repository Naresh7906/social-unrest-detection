{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tw_srch.data_extraction import data_extract\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import datetime\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    user=\"chordio\",\n",
    "    password=\"chordio\",\n",
    "    database=\"social_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(id,data,author,loc,cat,date):\n",
    "    mycursor = mydb.cursor()\n",
    "    edate = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    parameters = [id,data,author,loc,cat,edate]\n",
    "    insert_ur = \"INSERT INTO `data`(`tweet_id`,`data`,`author_id`,`location`,`category`,`date`) values((%s),(%s),(%s),(%s),(%s),(%s))\"\n",
    "    mycursor.execute(insert_ur,parameters)\n",
    "    mydb.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model-v2.h5')\n",
    "with open('tokenizer.pickle','rb') as token:\n",
    "    tokenizer = pickle.load(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('keywords')\n",
    "dxt = data_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {}\n",
    "for file in files:\n",
    "    key_file = open(f'keywords/{file}')\n",
    "    keywords = key_file.read().split('\\n')\n",
    "    category[str(file[9:-4])] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad_sequences(text):\n",
    "    max_len=50\n",
    "    val = [0,1]\n",
    "    xt = tokenizer.texts_to_sequences(text)\n",
    "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
    "    yt = model.predict(xt).argmax(axis=1)\n",
    "    return val[yt[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    for cat in category:\n",
    "        for words in category[cat]:\n",
    "            results = dxt.get_data(words)\n",
    "            for result in results:\n",
    "                relevence = tokenize_pad_sequences([str(result[1])])\n",
    "                if relevence == 0:\n",
    "                    continue\n",
    "                add_data(id=result[0],data=result[1],author=result[2],loc=result[3],date=result[4],cat=cat)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df642eef7b43fe7a4a517bca7a97690968a466268416ac555ac71584a4a4c66c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
